# BikeMonitor System - Comprehensive Research Documentation

**Generated:** 2025-11-30
**System:** Raspberry Pi 5 + Hailo-8 AI Accelerator
**Application:** Real-time bike theft prevention with AI person detection

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [Hardware Architecture](#hardware-architecture)
3. [Software Stack](#software-stack)
4. [Performance Analysis](#performance-analysis)
5. [Current System Status](#current-system-status)
6. [Optimization Roadmap](#optimization-roadmap)
7. [Implementation Guide](#implementation-guide)
8. [Troubleshooting](#troubleshooting)
9. [References](#references)

---

## Executive Summary

### System Overview
BikeMonitor is a real-time AI-powered theft prevention system using:
- **Hardware**: Raspberry Pi 5 with Hailo-8 AI accelerator (26 TOPS)
- **AI Model**: YOLOv8n for person detection
- **Camera**: Reolink RTSP camera (H.265/HEVC stream)
- **Alerts**: Pushover notifications + Twilio voice calls
- **Zone Monitoring**: Polygon-based intrusion detection with timer

### Key Research Findings

1. **NumPy Compatibility Issue (CRITICAL)**
   - NumPy 2.0+ is incompatible with HailoRT
   - **Solution**: Downgrade to NumPy 1.23.3
   - Symptom: "Memory size mismatch" errors

2. **Model Compatibility**
   - Your device: Hailo-8 (26 TOPS)
   - Current model: yolov8n.hef compiled for Hailo-8L (13 TOPS)
   - Impact: ~50% performance loss
   - **Recommendation**: Download Hailo-8 optimized model

3. **Performance Optimization Opportunity**
   - Current: ~30 FPS (batch size 1)
   - Optimized: ~120-137 FPS (batch size 8)
   - **4x performance improvement available**

4. **PCIe Configuration**
   - Gen 3 enabled: 2x performance vs Gen 2
   - Current setting: Correct (dtparam=pciex1_gen=3)

---

## Hardware Architecture

### Raspberry Pi 5 Specifications

| Component | Specification | Notes |
|-----------|--------------|-------|
| **CPU** | Broadcom BCM2712 (Quad-core Cortex-A76 @ 2.4GHz) | 2-3x faster than Pi 4 |
| **RAM** | 8GB LPDDR4X-4267 | Sufficient for AI workloads |
| **PCIe** | 1x PCIe 2.0 lane (Gen 3 with modification) | Connects to Hailo-8 |
| **GPU** | VideoCore VII | No H.264 decode, Has H.265 decode |
| **Power** | 27W USB-C | Required for M.2 HAT |

### Hailo-8 AI Accelerator

| Feature | Specification | vs Hailo-8L |
|---------|--------------|-------------|
| **TOPS** | 26 | 2x (8L has 13 TOPS) |
| **Architecture** | Hailo-8 | Different HEF compilation |
| **Interface** | PCIe Gen 3.0 (M.2 Key M, 4 lanes capable) | 8L: 2 lanes max |
| **Power** | ~2.5W typical | More efficient than GPU |
| **Models** | Pre-compiled HEF files | NOT compatible with 8L HEFs |

### PCIe Performance

**Gen 2 vs Gen 3 Impact:**
- **Gen 2.0** (default): 5 GT/sec â†’ ~30 FPS
- **Gen 3.0** (enabled): 8 GT/sec â†’ ~60 FPS
- **Bandwidth**: 2x improvement with Gen 3

**Your Configuration** (âœ… Correct):
```bash
# /boot/firmware/config.txt
dtparam=pciex1_gen=3
```

**Verification Command:**
```bash
lspci -vv | grep "LnkSta:"
# Should show: Speed 8GT/s (Gen 3)
```

### Thermal Management

**Critical Thresholds:**
- **80-85Â°C**: Progressive ARM throttling begins
- **85Â°C**: Full throttling (ARM + GPU)
- **Active Cooler**: Essential for 24/7 operation

**Monitoring:**
```bash
# Temperature
vcgencmd measure_temp

# Throttling status
vcgencmd get_throttled
# 0x0 = No throttling
# 0x50000 = Throttled previously
```

**Recommendations:**
- âœ… Install Raspberry Pi Active Cooler (35-40 dB)
- âœ… Add thermal pad to Hailo-8 module
- âœ… Ensure ventilation if in enclosure

---

## Software Stack

### HailoRT Architecture

```
Application (Python)
        â†“
hailo-platform (Python Bindings)
        â†“
HailoRT (C++ Library)
        â†“
Kernel Driver
        â†“
Hailo-8 Hardware (PCIe)
```

### Version Compatibility Matrix

| Component | Version | Status | Notes |
|-----------|---------|--------|-------|
| **Python** | 3.11 | âœ… Supported | Your environment |
| **NumPy** | 2.2.6 | âŒ **INCOMPATIBLE** | Must use 1.23.3 |
| **HailoRT** | 4.18+ | âœ… Recommended | Check with `hailortcli` |
| **OpenCV** | 4.x | âœ… Compatible | cv2.VideoCapture |
| **TAPPAS** | 3.30.0+ | âš ï¸ Optional | For GStreamer pipelines |

### Python API Usage Patterns

**Correct Implementation (Based on Research):**

```python
import numpy as np
from hailo_platform import (HEF, VDevice, ConfigureParams,
                             FormatType, HailoStreamInterface,
                             InferVStreams, InputVStreamParams,
                             OutputVStreamParams)

# 1. Load HEF model
hef = HEF("yolov8n.hef")

with VDevice() as target:
    # 2. Configure network
    configure_params = ConfigureParams.create_from_hef(
        hef,
        interface=HailoStreamInterface.PCIe
    )
    network_group = target.configure(hef, configure_params)[0]
    network_group_params = network_group.create_params()

    # 3. Get input/output info from HEF
    input_vstream_info = hef.get_input_vstream_infos()[0]
    output_vstream_info = hef.get_output_vstream_infos()[0]

    # 4. Create VStream params
    input_vstreams_params = InputVStreamParams.make_from_network_group(
        network_group,
        quantized=False,  # Auto quantization
        format_type=FormatType.FLOAT32
    )
    output_vstreams_params = OutputVStreamParams.make_from_network_group(
        network_group,
        quantized=False,  # Auto dequantization
        format_type=FormatType.FLOAT32
    )

    # 5. Get expected shape
    input_shape = input_vstream_info.shape
    print(f"Model expects: {input_shape}")  # e.g., (640, 640, 3)

    # 6. Activate network group (CRITICAL!)
    with network_group.activate(network_group_params):
        # 7. Create inference pipeline
        with InferVStreams(network_group, input_vstreams_params, output_vstreams_params) as infer_pipeline:
            # 8. Prepare input
            frame = cv2.imread("image.jpg")
            input_data = cv2.resize(frame, (640, 640))
            input_data = cv2.cvtColor(input_data, cv2.COLOR_BGR2RGB)
            input_data = input_data.astype(np.float32) / 255.0
            input_data = np.expand_dims(input_data, axis=0)  # Add batch dimension

            # 9. Run inference
            input_dict = {input_vstream_info.name: input_data}
            output_dict = infer_pipeline.infer(input_dict)

            # 10. Get results
            output_data = output_dict[output_vstream_info.name]
```

**Key Points:**
- âœ… Use `hef.get_input_vstream_infos()` for metadata
- âœ… Must activate network_group before InferVStreams
- âœ… Use vstream_info.name for dictionary keys
- âœ… Add batch dimension even for single frame
- âŒ Don't use dictionary indexing on params (use .name)

---

## Performance Analysis

### YOLOv8 Benchmarks on Hailo-8

**Official Benchmarks (640x640, Batch Size 8):**

| Model | FPS | TOPS Used | Use Case |
|-------|-----|-----------|----------|
| **YOLOv8n** | 136.7 | ~3 TOPS | Lightweight, your model |
| **YOLOv8s** | 127.85 | ~4 TOPS | Better accuracy |
| **YOLOv6n** | 354.07 | ~2 TOPS | Ultra-fast alternative |
| **YOLOv5s** | 150.21 | ~3.5 TOPS | Face detection variant |

**Your Current Setup:**
- Model: YOLOv8n (Hailo-8L compiled) âš ï¸ Wrong hardware
- Batch Size: 1 âš ï¸ Not optimized
- **Expected FPS**: ~30 FPS

**Optimized Setup:**
- Model: YOLOv8n (Hailo-8 compiled) âœ…
- Batch Size: 8 âœ…
- **Expected FPS**: ~136 FPS

### Batch Size Impact

**Research Findings (Hailo-8L on Pi 5):**

| Batch Size | FPS | PCIe Utilization | Notes |
|------------|-----|------------------|-------|
| 1 | 30 | Low | Current setup |
| 2 | 80 | Medium | 2.6x improvement |
| 4 | 100 | High | 3.3x improvement |
| **8** | **120** | **Optimal** | **4x improvement** |
| 16 | 100 | Saturated | PCIe bottleneck |
| 32 | 54 | Overloaded | Performance drops |

**Conclusion**: Batch size 8 is optimal for PCIe Gen 3 bandwidth.

### CPU Usage Comparison

**Different Implementation Approaches:**

| Method | FPS | CPU Usage | Complexity |
|--------|-----|-----------|------------|
| CPU Only (Ultralytics) | 0.75 | 95% | Low |
| Hailo Single-threaded | 30 | 60-70% | Low |
| **Hailo Batched** | **120** | **40-50%** | **Medium** |
| Hailo + GStreamer | 150+ | 20-30% | High |

**Your Current**: Single-threaded â†’ **Optimization opportunity!**

### Memory Footprint

| Component | Memory Usage | Notes |
|-----------|-------------|-------|
| Model (HEF) | ~12 MB | Pre-loaded |
| Input Buffer (1 frame) | ~1.2 MB | 640x640x3xfloat32 |
| Input Buffer (8 frames) | ~9.6 MB | Batched |
| Output Buffer | ~1 MB | Detections |
| **Total Runtime** | **~50 MB** | Very efficient |

---

## Current System Status

### Implementation Analysis

**File**: `/home/pi5/Desktop/BikeMonitor/hailo_theft_prevention.py`

#### âœ… What's Working

1. **VDevice Management**: Correct context manager usage
2. **HEF Loading**: Proper model loading
3. **PCIe Interface**: Correctly configured
4. **Network Activation**: Now properly activated (fixed)
5. **Stream Info**: Using vstream_info.name (fixed)
6. **Zone Detection**: Custom polygon-based intrusion detection
7. **SimpleTracker**: Lightweight object tracking implementation
8. **Alert System**: Pushover + Twilio integration

#### âš ï¸ Current Issues

1. **NumPy Version**: 2.2.6 (MUST downgrade to 1.23.3)
2. **Wrong Model**: Using Hailo-8L model on Hailo-8 hardware
3. **No Batching**: Processing 1 frame at a time
4. **Synchronous Pipeline**: Captureâ†’Inferenceâ†’Display in sequence
5. **No Hardware Decode**: CPU-based HEVC decoding

#### ðŸ“Š Performance Estimate

**Current Setup:**
- FPS: ~30 (with NumPy fix)
- CPU: ~60-70%
- Latency: 450ms-1000ms (TCP drift)

**After All Optimizations:**
- FPS: ~120-137
- CPU: ~30-40%
- Latency: ~500ms (stable)

---

## Optimization Roadmap

### Phase 1: Critical Fixes (DO NOW)

#### 1.1 Fix NumPy Version âš¡ **HIGHEST PRIORITY**

**Problem**: NumPy 2.2.6 causes memory mismatch errors
**Solution**:
```bash
pip3 install numpy==1.23.3
```

**Verification**:
```bash
python3 -c "import numpy; print(numpy.__version__)"
# Should output: 1.23.3
```

**Impact**: Fixes crashes, enables system to run

---

#### 1.2 Download Correct Hailo-8 Model

**Problem**: Using Hailo-8L model â†’ 50% performance loss
**Solution**:
```bash
cd ~/Desktop/BikeMonitor
wget https://hailo-model-zoo.s3.eu-west-2.amazonaws.com/ModelZoo/Compiled/v2.12.0/hailo8/yolov8n.hef
```

**Impact**: 2x performance improvement (30 â†’ 60 FPS)

---

### Phase 2: Performance Optimizations (HIGH VALUE)

#### 2.1 Implement Batch Inference (Batch Size 8)

**Current Code** (line 186-200):
```python
# Single frame processing
input_data = np.expand_dims(input_data, axis=0)  # Batch of 1
input_dict = {input_name: input_data}
output_dict = infer_pipeline.infer(input_dict)
```

**Optimized Code**:
```python
# Batch processing (add after line 184)
frame_buffer = []
BATCH_SIZE = 8

while True:
    ret, frame = cap.read()
    if not ret:
        break

    # Prepare frame
    input_frame = cv2.resize(frame, (640, 640))
    input_frame = cv2.cvtColor(input_frame, cv2.COLOR_BGR2RGB)
    input_frame = input_frame.astype(np.float32) / 255.0
    frame_buffer.append((input_frame, frame))  # Store original too

    # Process when buffer is full
    if len(frame_buffer) == BATCH_SIZE:
        # Stack frames into batch
        batch_input = np.array([f[0] for f in frame_buffer])

        # Run batched inference
        input_dict = {input_name: batch_input}
        output_dict = infer_pipeline.infer(input_dict)
        output_data = output_dict[output_name]

        # Process each frame in batch
        for i, (_, original_frame) in enumerate(frame_buffer):
            frame_output = output_data[i]  # Get this frame's results
            detections = postprocess_yolo(
                np.expand_dims(frame_output, 0),  # Add batch dim back
                CONFIDENCE_THRESHOLD,
                frame_width,
                frame_height
            )
            # ... rest of processing ...

        frame_buffer = []  # Clear buffer
```

**Impact**: 4x FPS improvement (30 â†’ 120 FPS)

**Trade-offs**:
- âœ… Pro: Massive performance gain
- âœ… Pro: Better PCIe utilization
- âš ï¸ Con: 8-frame latency (~266ms at 30fps input)
- âš ï¸ Con: More complex code

**Recommendation**: Implement for production, latency acceptable for theft detection

---

#### 2.2 Add Periodic RTSP Reconnection

**Problem**: TCP latency increases over time (450ms â†’ 1000ms+ after 15 min)
**Solution** (add after line 178):

```python
# Add at top of file
RECONNECT_INTERVAL = 900  # 15 minutes
last_reconnect = time.time()

# Inside main loop (before ret, frame = cap.read())
while True:
    # Check if reconnection needed
    current_time = time.time()
    if current_time - last_reconnect > RECONNECT_INTERVAL:
        print("[INFO] Reconnecting to prevent TCP latency buildup...")
        cap.release()
        cap = cv2.VideoCapture(RTSP_URL, cv2.CAP_FFMPEG)
        last_reconnect = current_time
        # Wait for stream to stabilize
        for _ in range(10):
            cap.read()

    ret, frame = cap.read()
    # ... rest of code ...
```

**Impact**: Stable 500ms latency (prevents drift to 1000ms+)

---

### Phase 3: Advanced Optimizations (MEDIUM VALUE)

#### 3.1 Hardware-Accelerated HEVC Decoding

**Step 1: Check Camera Codec**
```bash
ffprobe rtsp://Dair:A123456a\!@80.178.150.137:554/unicast/c1/s0/live
```

**Look for**: `Video: hevc` or `Video: h264`

**If H.265/HEVC (RECOMMENDED)**:

Replace OpenCV capture (line 149-157) with:

```python
import subprocess

def create_hw_accelerated_capture(rtsp_url, frame_width, frame_height):
    """Create FFmpeg process with hardware-accelerated HEVC decoding"""
    ffmpeg_cmd = [
        'ffmpeg',
        '-hwaccel', 'drm',  # Pi 5 hardware acceleration
        '-rtsp_transport', 'tcp',
        '-i', rtsp_url,
        '-f', 'rawvideo',
        '-pix_fmt', 'bgr24',
        '-'
    ]

    process = subprocess.Popen(
        ffmpeg_cmd,
        stdout=subprocess.PIPE,
        stderr=subprocess.DEVNULL,
        bufsize=10**8
    )

    return process, frame_width * frame_height * 3

# Usage (replace line 149-157)
print(f"Opening video stream: {RTSP_URL}")
ffmpeg_process, frame_size = create_hw_accelerated_capture(RTSP_URL, 1920, 1080)

# Wait for stream to stabilize
time.sleep(2)

# In main loop (replace cap.read())
raw_frame = ffmpeg_process.stdout.read(frame_size)
if len(raw_frame) != frame_size:
    # Restart FFmpeg process
    ffmpeg_process.kill()
    ffmpeg_process, frame_size = create_hw_accelerated_capture(RTSP_URL, 1920, 1080)
    continue

frame = np.frombuffer(raw_frame, dtype=np.uint8).reshape((1080, 1920, 3))
```

**Impact**: CPU usage 60-70% â†’ 20-30%

**If H.264**: Pi 5 has no hardware decode, consider:
- Configure camera to H.265 if supported
- Use substream (lower resolution, lower CPU)
- Accept higher CPU usage

---

#### 3.2 Multi-Threaded Pipeline

**Architecture**:
```
Thread 1: Frame Capture â†’ Queue
Thread 2: Batch Inference â†’ Queue
Thread 3: Post-processing & Display
```

**Implementation** (complex, ~150 lines):

```python
import threading
import queue

# Queues
capture_queue = queue.Queue(maxsize=16)
inference_queue = queue.Queue(maxsize=8)
result_queue = queue.Queue(maxsize=16)

def capture_thread(rtsp_url):
    """Continuously capture frames"""
    cap = cv2.VideoCapture(rtsp_url, cv2.CAP_FFMPEG)
    while True:
        ret, frame = cap.read()
        if ret:
            capture_queue.put(frame)

def inference_thread(infer_pipeline, input_name, output_name):
    """Batch inference thread"""
    batch_buffer = []
    BATCH_SIZE = 8

    while True:
        # Accumulate frames
        for _ in range(BATCH_SIZE):
            frame = capture_queue.get()
            preprocessed = preprocess_frame(frame)
            batch_buffer.append((preprocessed, frame))

        # Run batched inference
        batch = np.array([f[0] for f in batch_buffer])
        input_dict = {input_name: batch}
        output_dict = infer_pipeline.infer(input_dict)

        # Send results
        for i, (_, original) in enumerate(batch_buffer):
            result_queue.put((original, output_dict[output_name][i]))

        batch_buffer = []

def display_thread():
    """Post-process and display"""
    while True:
        frame, detections = result_queue.get()
        # ... tracking, zone detection, display ...

# Start threads
t1 = threading.Thread(target=capture_thread, args=(RTSP_URL,), daemon=True)
t2 = threading.Thread(target=inference_thread, args=(infer_pipeline, input_name, output_name), daemon=True)
t3 = threading.Thread(target=display_thread, daemon=True)

t1.start()
t2.start()
t3.start()

# Keep main thread alive
t1.join()
```

**Impact**:
- Better CPU utilization
- Smoother frame rate
- More complex debugging

**Recommendation**: Implement ONLY if single-threaded batching insufficient

---

### Phase 4: System Hardening (LOW PRIORITY)

#### 4.1 Temperature Monitoring

Add monitoring service:

```python
import subprocess

def check_thermal_status():
    """Monitor temperature and throttling"""
    temp_output = subprocess.check_output(['vcgencmd', 'measure_temp']).decode()
    temp = float(temp_output.split('=')[1].split("'")[0])

    throttled_output = subprocess.check_output(['vcgencmd', 'get_throttled']).decode()
    throttled = throttled_output.split('=')[1].strip()

    if temp > 80:
        print(f"[WARNING] High temperature: {temp}Â°C")
    if throttled != '0x0':
        print(f"[WARNING] Throttling detected: {throttled}")

    return temp, throttled

# Add to main loop (every 60 seconds)
last_thermal_check = 0
if time.time() - last_thermal_check > 60:
    check_thermal_status()
    last_thermal_check = time.time()
```

---

#### 4.2 Systemd Watchdog

Create `/etc/systemd/system/bikemonitor.service`:

```ini
[Unit]
Description=Bike Monitor Theft Prevention
After=network.target

[Service]
Type=notify
User=pi
WorkingDirectory=/home/pi/Desktop/BikeMonitor
ExecStart=/usr/bin/python3 /home/pi/Desktop/BikeMonitor/hailo_theft_prevention.py
Restart=on-failure
RestartSec=10
WatchdogSec=60

[Install]
WantedBy=multi-user.target
```

Add to Python code:
```python
import systemd.daemon

# In main loop
systemd.daemon.notify('WATCHDOG=1')
```

---

## Implementation Guide

### Quick Start (Get System Running)

**Step 1: Fix NumPy**
```bash
pip3 install numpy==1.23.3
```

**Step 2: Test Current Code**
```bash
cd ~/Desktop/BikeMonitor
python3 hailo_theft_prevention.py
```

**Expected**: System runs at ~30 FPS without crashes

---

**Step 3: Download Correct Model**
```bash
wget https://hailo-model-zoo.s3.eu-west-2.amazonaws.com/ModelZoo/Compiled/v2.12.0/hailo8/yolov8n.hef
python3 hailo_theft_prevention.py
```

**Expected**: ~60 FPS (2x improvement)

---

**Step 4: Implement Batch Inference**

Create new file: `hailo_theft_prevention_optimized.py`

Copy existing code and modify inference loop to use batching (see Phase 2.1)

**Expected**: ~120 FPS (4x improvement)

---

**Step 5: Add RTSP Reconnection**

Add reconnection logic (see Phase 2.2)

**Expected**: Stable latency over 24+ hours

---

### Full Production Deployment

**Step 1: Hardware Verification**
```bash
# Check PCIe Gen 3
lspci -vv | grep "LnkSta:" | head -1
# Should show: Speed 8GT/s

# Check Hailo device
hailortcli fw-control identify
# Should show: Hailo-8 device

# Check temperature
vcgencmd measure_temp
# Should be <80Â°C
```

---

**Step 2: Software Configuration**

Create `.env` file:
```bash
cp .env.example .env
nano .env
```

Add credentials:
```
PUSHOVER_USER_KEY=your_key_here
PUSHOVER_APP_TOKEN=your_token_here
TWILIO_ACCOUNT_SID=your_sid_here
TWILIO_AUTH_TOKEN=your_token_here
TWILIO_PHONE_NUMBER=+1234567890
YOUR_PHONE_NUMBER=+1987654321
```

---

**Step 3: Test Alert System**
```bash
python3 alert_service.py
```

Verify you receive:
- âœ… Pushover notification
- âœ… Twilio voice call

---

**Step 4: Zone Configuration**

Run zone selector:
```bash
python3 zone_selector.py
```

Update `ZONE_POLYGON` in `hailo_theft_prevention.py` with new coordinates

---

**Step 5: 24-Hour Stress Test**

```bash
# Run in tmux/screen session
tmux new -s bikemonitor
python3 hailo_theft_prevention_optimized.py

# Detach: Ctrl+B, then D
# Reattach: tmux attach -t bikemonitor
```

Monitor logs for:
- Memory leaks
- Thermal throttling
- RTSP disconnections
- Alert accuracy

---

**Step 6: Production Deployment**

Enable systemd service:
```bash
sudo cp bikemonitor.service /etc/systemd/system/
sudo systemctl daemon-reload
sudo systemctl enable bikemonitor
sudo systemctl start bikemonitor
```

Monitor:
```bash
sudo systemctl status bikemonitor
journalctl -u bikemonitor -f
```

---

## Troubleshooting

### Error: "Memory size mismatch"

**Symptoms**:
```
[HailoRT] [error] CHECK failed - Memory size of vstream yolov8n/input_layer1
does not match the frame count! (Expected 4915200, got 0)
```

**Root Cause**: NumPy 2.0+ incompatibility

**Solution**:
```bash
pip3 install numpy==1.23.3
python3 -c "import numpy; print(numpy.__version__)"  # Verify 1.23.3
```

---

### Error: "Driver version mismatch"

**Symptoms**:
```
Driver version is different from library version
(Driver: 4.20.0, Library: 4.21.0)
```

**Solution**:
```bash
sudo apt remove --purge hailo-all
sudo apt autoremove
sudo reboot
# Re-run Hailo installation script
```

---

### Low FPS (<10)

**Diagnosis**:

1. **Check PCIe Gen**:
```bash
lspci -vv | grep "LnkSta:" | head -1
```
- If shows Gen 2, enable Gen 3 in config.txt

2. **Check Model Compatibility**:
```bash
ls -lh yolov8n.hef
```
- Ensure using Hailo-8 model, not Hailo-8L

3. **Check CPU Usage**:
```bash
htop
```
- If >90%, implement batching or hardware decode

4. **Benchmark Hardware**:
```bash
hailortcli benchmark yolov8n.hef --batch-size 8
```
- Should show >100 FPS
- If not, hardware issue

---

### RTSP Stream Issues

**Gray Screen / Freezing**:

1. **Check codec**:
```bash
ffprobe rtsp://your_url
```

2. **Try substream**:
```python
RTSP_URL = "rtsp://Dair:A123456a%21@80.178.150.137:554/unicast/c1/s1/live"
# s1 = substream (lower resolution)
```

3. **Increase buffer**:
```python
os.environ["OPENCV_FFMPEG_CAPTURE_OPTIONS"] = (
    "rtsp_transport;tcp|buffer_size;10240000"
)
```

---

### Thermal Throttling

**Symptoms**: FPS drops over time, system sluggish

**Check**:
```bash
vcgencmd measure_temp
vcgencmd get_throttled
```

**Solutions**:
1. Install Active Cooler
2. Improve ventilation
3. Reduce workload (lower resolution, batch size)

---

### Alerts Not Sending

**Test Individually**:
```bash
python3 alert_service.py
```

**Pushover Issues**:
- Verify User Key and App Token in `.env`
- Check quota: https://pushover.net/apps

**Twilio Issues**:
- Verify SID, Token, phone numbers
- Check balance: https://console.twilio.com/
- Ensure phone numbers include country code (+1)

---

## References

### Official Documentation

1. **Hailo**
   - [Hailo-8 Product Page](https://hailo.ai/products/ai-accelerators/hailo-8-ai-accelerator/)
   - [HailoRT GitHub](https://github.com/hailo-ai/hailort)
   - [Hailo Model Zoo](https://github.com/hailo-ai/hailo_model_zoo)
   - [TAPPAS Framework](https://github.com/hailo-ai/tappas)

2. **Raspberry Pi**
   - [AI Kit Documentation](https://www.raspberrypi.com/documentation/accessories/ai-kit.html)
   - [Pi 5 Examples](https://github.com/hailo-ai/hailo-rpi5-examples)
   - [PCIe Configuration](https://www.raspberrypi.com/documentation/computers/raspberry-pi.html#pcie-gen-3-0)

3. **Community Resources**
   - [Hailo Community Forum](https://community.hailo.ai/)
   - [Raspberry Pi Forums](https://forums.raspberrypi.com/)

### Key Research Papers

1. **Hailo-8 vs Hailo-8L Comparison**
   - Community: https://community.hailo.ai/t/what-are-differences-between-hailo-8-and-hailo-8l/1675

2. **Performance Benchmarks**
   - RPi Forums: https://forums.raspberrypi.com/viewtopic.php?t=373867
   - Seeed Studio: https://wiki.seeedstudio.com/benchmark_on_rpi5_and_cm4_running_yolov8s_with_rpi_ai_kit/

3. **NumPy Compatibility**
   - Community: https://community.hailo.ai/t/memory-size-frame-count-mismatch/1338

4. **RTSP Latency Analysis**
   - Medium: https://gektor650.medium.com/comparing-video-stream-latencies-raspberry-pi-5-camera-v3-a8d5dad2f67b

5. **Hardware Acceleration**
   - Frigate Discussion: https://github.com/blakeblackshear/frigate/discussions/18431

---

## Appendix: Performance Optimization Summary

### Current System (Before Optimization)

| Metric | Value |
|--------|-------|
| FPS | 30 |
| CPU Usage | 60-70% |
| Latency | 450-1000ms |
| Model | Hailo-8L (wrong) |
| Batch Size | 1 |
| NumPy | 2.2.6 (broken) |

### Optimized System (After All Changes)

| Metric | Value | Improvement |
|--------|-------|-------------|
| FPS | 120-137 | **4.5x** |
| CPU Usage | 30-40% | **40% reduction** |
| Latency | 500ms | **Stable** |
| Model | Hailo-8 | âœ… Correct |
| Batch Size | 8 | âœ… Optimal |
| NumPy | 1.23.3 | âœ… Compatible |

### Optimization Impact Breakdown

| Optimization | FPS Gain | Effort | Priority |
|--------------|----------|--------|----------|
| Fix NumPy | +âˆž | 1 min | ðŸ”´ Critical |
| Correct Model | 2x | 2 min | ðŸ”´ Critical |
| Batch Size 8 | 2x | 1 hour | ðŸŸ¡ High |
| HW Decode | +0% FPS, -30% CPU | 2 hours | ðŸŸ¢ Medium |
| Multi-threading | +10-20% | 4 hours | ðŸ”µ Low |
| RTSP Reconnect | Stability | 15 min | ðŸŸ¡ High |

**Recommended Order**:
1. Fix NumPy (1 min) ðŸ”´
2. Download Hailo-8 model (2 min) ðŸ”´
3. Test basic system (verify it works)
4. Implement batching (1 hour) ðŸŸ¡
5. Add RTSP reconnection (15 min) ðŸŸ¡
6. Test 24-hour stability
7. (Optional) Hardware decode (2 hours) ðŸŸ¢
8. (Optional) Multi-threading (4 hours) ðŸ”µ

---

**End of Documentation**

*Last Updated: 2025-11-30*
*System Version: BikeMonitor v1.0*
*Hardware: Raspberry Pi 5 + Hailo-8*
